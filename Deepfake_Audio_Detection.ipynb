{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "CfvpV_YKsKSC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 785,
     "referenced_widgets": [
      "f7b7fb19748b4561a834b4bf6b3d0ce3",
      "5f3289b9e647441a99b63856f27a91c2",
      "8350bf7698ab4dee9245001b4af1cffe",
      "669a67ddca194836886f88749a73f273",
      "7e7f023c5ae54dd89797d51807cd6e59",
      "a1e4d061962a46e29b75b7e92dad6aa1",
      "5b6cd09fefec440d88def3da19c6f3e6",
      "f86681912ee745808ed365a011a13abd",
      "56e8bcb7aa22452290361af5968a80ea",
      "2e0d036b3dcc4538949d30c1bcd50a42",
      "c69f52328f16428a985adfbade63bd96"
     ]
    },
    "id": "CfvpV_YKsKSC",
    "outputId": "a8e29d54-f5d0-4555-f524-838a0b895ef2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7b7fb19748b4561a834b4bf6b3d0ce3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/6794 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded: dict_keys(['audio'])\n",
      "\n",
      "ðŸ§  svm_audio_model.pkl Results:\n",
      "Accuracy: 0.65\n",
      "Precision: 0.5384615384615384\n",
      "Recall: 0.875\n",
      "F1-Score: 0.6666666666666666\n",
      "AUC-ROC: 0.7604166666666667\n",
      "âœ… Saved: svm_audio_model.pkl\n",
      "\n",
      "ðŸ§  lr_audio_model.pkl Results:\n",
      "Accuracy: 0.8\n",
      "Precision: 0.75\n",
      "Recall: 0.75\n",
      "F1-Score: 0.75\n",
      "AUC-ROC: 0.9583333333333334\n",
      "âœ… Saved: lr_audio_model.pkl\n",
      "\n",
      "ðŸ§  perceptron_audio_model.pkl Results:\n",
      "Accuracy: 0.75\n",
      "Precision: 0.7142857142857143\n",
      "Recall: 0.625\n",
      "F1-Score: 0.6666666666666666\n",
      "AUC-ROC: 0.7291666666666667\n",
      "âœ… Saved: perceptron_audio_model.pkl\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§  Deep Neural Network Results:\n",
      "Accuracy: 0.6\n",
      "Precision: 0.5\n",
      "Recall: 0.875\n",
      "F1-Score: 0.6363636363636364\n",
      "AUC-ROC: 0.6145833333333334\n",
      "âœ… Saved: dnn_audio_model.h5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import time\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# --- 1. Load Dataset (Limited Subset for Safety) ---\n",
    "try:\n",
    "    dataset = load_dataset(\"CSALT/deepfake_detection_dataset_urdu\", split=\"train[:100]\")\n",
    "    print(\"Dataset Loaded:\", dataset[0].keys())\n",
    "except Exception as e:\n",
    "    print(\"Failed to load dataset:\", e)\n",
    "    exit()\n",
    "\n",
    "# --- 2. Feature Extraction ---\n",
    "def extract_mfcc(audio_array, sr):\n",
    "    mfcc = librosa.feature.mfcc(y=audio_array, sr=sr, n_mfcc=13)\n",
    "    return np.mean(mfcc.T, axis=0)\n",
    "\n",
    "X = []\n",
    "\n",
    "for i, item in enumerate(dataset):\n",
    "    try:\n",
    "        audio_array = item[\"audio\"][\"array\"]\n",
    "        sr = item[\"audio\"][\"sampling_rate\"]\n",
    "        mfcc = extract_mfcc(audio_array, sr)\n",
    "        X.append(mfcc)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing sample {i}: {e}\")\n",
    "    time.sleep(1)  # Avoid rate limits\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array([0 if i < len(X) // 2 else 1 for i in range(len(X))])  # Dummy binary labels\n",
    "\n",
    "# --- 3. Train-Test Split ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- 4. Train and Save Classical Models ---\n",
    "models = {\n",
    "    \"svm_audio_model.pkl\": SVC(probability=True),\n",
    "    \"lr_audio_model.pkl\": LogisticRegression(max_iter=1000),\n",
    "    \"perceptron_audio_model.pkl\": Perceptron()\n",
    "}\n",
    "\n",
    "for filename, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else y_pred\n",
    "\n",
    "    print(f\"\\nðŸ§  {filename} Results:\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "    print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "    print(\"F1-Score:\", f1_score(y_test, y_pred))\n",
    "    print(\"AUC-ROC:\", roc_auc_score(y_test, y_prob))\n",
    "\n",
    "    # âœ… Save model using joblib\n",
    "    joblib.dump(model, filename)\n",
    "    print(f\"âœ… Saved: {filename}\")\n",
    "\n",
    "# --- 5. Deep Neural Network (DNN) ---\n",
    "dnn = Sequential([\n",
    "    Dense(64, input_shape=(X.shape[1],), activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "dnn.compile(optimizer=Adam(learning_rate=0.001),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "dnn.fit(X_train, y_train, epochs=20, batch_size=16, verbose=0)\n",
    "\n",
    "y_pred_dnn = (dnn.predict(X_test) > 0.5).astype(int).flatten()\n",
    "y_prob_dnn = dnn.predict(X_test).flatten()\n",
    "\n",
    "print(\"\\nðŸ§  Deep Neural Network Results:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_dnn))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_dnn))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_dnn))\n",
    "print(\"F1-Score:\", f1_score(y_test, y_pred_dnn))\n",
    "print(\"AUC-ROC:\", roc_auc_score(y_test, y_prob_dnn))\n",
    "\n",
    "# âœ… Save DNN model\n",
    "dnn.save(\"dnn_audio_model.h5\")\n",
    "print(\"âœ… Saved: dnn_audio_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MIsHUWnehbJl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MIsHUWnehbJl",
    "outputId": "77824070-35ce-450e-c4b8-dc422a9d0950"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1386 entries, 0 to 1385\n",
      "Data columns (total 8 columns):\n",
      " #   Column                   Non-Null Count  Dtype \n",
      "---  ------                   --------------  ----- \n",
      " 0   report                   1386 non-null   object\n",
      " 1   type_blocker             1386 non-null   int64 \n",
      " 2   type_regression          1386 non-null   int64 \n",
      " 3   type_bug                 1386 non-null   int64 \n",
      " 4   type_documentation       1386 non-null   int64 \n",
      " 5   type_enhancement         1386 non-null   int64 \n",
      " 6   type_task                1386 non-null   int64 \n",
      " 7   type_dependency_upgrade  1386 non-null   int64 \n",
      "dtypes: int64(7), object(1)\n",
      "memory usage: 86.8+ KB\n",
      "None\n",
      "                                              report  type_blocker  \\\n",
      "0  The mention of Fix Super Stream Example in Doc...             0   \n",
      "1  It seems like you need a concise summary relat...             0   \n",
      "2  The issue AMQP 838 opened by Gary Russell invo...             0   \n",
      "3  I m unable to access external content directly...             0   \n",
      "4  In the discussion around AMQP 815 https jira s...             0   \n",
      "\n",
      "   type_regression  type_bug  type_documentation  type_enhancement  type_task  \\\n",
      "0                0         1                   1                 0          0   \n",
      "1                0         1                   1                 0          0   \n",
      "2                0         1                   1                 0          0   \n",
      "3                0         1                   1                 0          0   \n",
      "4                0         1                   1                 0          0   \n",
      "\n",
      "   type_dependency_upgrade  \n",
      "0                        0  \n",
      "1                        0  \n",
      "2                        0  \n",
      "3                        0  \n",
      "4                        0  \n",
      "Shape: (1386, 8)\n",
      "Missing values:\n",
      " report                     0\n",
      "type_blocker               0\n",
      "type_regression            0\n",
      "type_bug                   0\n",
      "type_documentation         0\n",
      "type_enhancement           0\n",
      "type_task                  0\n",
      "type_dependency_upgrade    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = \"dataset.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display basic information\n",
    "print(df.info())\n",
    "print(df.head())\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Missing values:\\n\", df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "poxmwtCZJkDE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "poxmwtCZJkDE",
    "outputId": "f30018b2-707e-458f-d471-881ac5766013"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-57db1854-d0a9-4a12-9621-49a66f3271b6\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-57db1854-d0a9-4a12-9621-49a66f3271b6\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataset.csv to dataset.csv\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "pcrD8IsgjVV3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pcrD8IsgjVV3",
    "outputId": "015d6d7d-b596-4f52-986b-b554d45afa36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset loaded successfully!\n",
      "âš ï¸ Warning: Label 'type_task' has no positive samples in training set.\n",
      "\n",
      "ðŸ§  Logistic Regression Evaluation\n",
      "Hamming Loss: 0.12170263788968826\n",
      "Micro-F1 Score: 0.8122109158186864\n",
      "Macro-F1 Score: 0.4456896227693783\n",
      "\n",
      "ðŸ§  SVM Evaluation\n",
      "Hamming Loss: 0.1157074340527578\n",
      "Micro-F1 Score: 0.8224471021159153\n",
      "Macro-F1 Score: 0.5142038183470409\n",
      "\n",
      "ðŸ§  Perceptron Evaluation\n",
      "Hamming Loss: 0.13729016786570744\n",
      "Micro-F1 Score: 0.7893284268629255\n",
      "Macro-F1 Score: 0.6186586356023609\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['saved_models/perceptron_text_model.pkl']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import hamming_loss, f1_score\n",
    "import os\n",
    "import warnings\n",
    "from joblib import dump\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- 1. Load Dataset ---\n",
    "df = pd.read_csv(\"/content/dataset.csv\")\n",
    "print(\"âœ… Dataset loaded successfully!\")\n",
    "\n",
    "# --- 2. Define Features and Labels ---\n",
    "X_text = df['report']\n",
    "y = df.iloc[:, 1:]  # Assuming binary labels start from column index 1\n",
    "\n",
    "# --- 3. Feature Extraction ---\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X = tfidf.fit_transform(X_text)\n",
    "\n",
    "# Save the TF-IDF vectorizer\n",
    "os.makedirs(\"saved_models\", exist_ok=True)\n",
    "dump(tfidf, \"saved_models/tfidf_vectorizer.pkl\")\n",
    "\n",
    "# --- 4. Train-Test Split ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- 5. Check for Labels with No Positive Examples in Training Set ---\n",
    "for col in y.columns:\n",
    "    if y_train[col].sum() == 0:\n",
    "        print(f\"âš ï¸ Warning: Label '{col}' has no positive samples in training set.\")\n",
    "\n",
    "# --- 6. Filter Labels with No Positives in Training Set ---\n",
    "y_train_filtered = y_train.loc[:, y_train.sum(axis=0) > 0]\n",
    "y_test_filtered = y_test[y_train_filtered.columns]\n",
    "\n",
    "# --- 7. Evaluation Function ---\n",
    "def evaluate_model(name, model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"\\nðŸ§  {name} Evaluation\")\n",
    "    print(\"Hamming Loss:\", hamming_loss(y_test, y_pred))\n",
    "    print(\"Micro-F1 Score:\", f1_score(y_test, y_pred, average=\"micro\", zero_division=0))\n",
    "    print(\"Macro-F1 Score:\", f1_score(y_test, y_pred, average=\"macro\", zero_division=0))\n",
    "\n",
    "# --- 8. Train and Save Models ---\n",
    "\n",
    "# Logistic Regression\n",
    "lr = OneVsRestClassifier(LogisticRegression(max_iter=1000))\n",
    "lr.fit(X_train, y_train_filtered)\n",
    "evaluate_model(\"Logistic Regression\", lr, X_test, y_test_filtered)\n",
    "dump(lr, \"saved_models/lr_text_model.pkl\")\n",
    "\n",
    "# SVM\n",
    "svm = OneVsRestClassifier(SVC(probability=True))\n",
    "svm.fit(X_train, y_train_filtered)\n",
    "evaluate_model(\"SVM\", svm, X_test, y_test_filtered)\n",
    "dump(svm, \"saved_models/svm_text_model.pkl\")\n",
    "\n",
    "# Perceptron\n",
    "perceptron = OneVsRestClassifier(Perceptron())\n",
    "perceptron.fit(X_train, y_train_filtered)\n",
    "evaluate_model(\"Perceptron\", perceptron, X_test, y_test_filtered)\n",
    "dump(perceptron, \"saved_models/perceptron_text_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ir6Fk1F9JnAp",
   "metadata": {
    "id": "ir6Fk1F9JnAp"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
